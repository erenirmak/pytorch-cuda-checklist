{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# PYTORCH - CUDA CHECKLIST NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys, os\n",
    "import subprocess\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command):\n",
    "    \"\"\"Runs a shell command and returns the output as a string.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command, \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.PIPE, \n",
    "            text=True, \n",
    "            shell=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            return f\"Error/Not Found (Code {result.returncode}): {result.stderr.strip()}\"\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Execution Failed: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## SYSTEM & VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Diagnostics initialized on: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version:    {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch Version:   {torch.__version__}\")\n",
    "print(f\"OpenMP Enabled:  {torch.backends.openmp.is_available()}\")\n",
    "print(f\"MKL Enabled:     {torch.backends.mkl.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### System CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvcc_output = run_command(\"nvcc --version\")\n",
    "if \"release\" in nvcc_output:\n",
    "    lines = nvcc_output.split('\\n')\n",
    "    version_line = [l for l in lines if \"release\" in l][0]\n",
    "    print(f\"System NVCC:     {version_line.strip()}\")\n",
    "    print(\"Supported compute capabilities:\\n  - {arch_list}\".format(arch_list = run_command(\"nvcc --list-gpu-arch\").replace('\\n', '\\n  - ')))\n",
    "    print(\"Supported GPU architectures:\\n  - {arch_list}\".format(arch_list = run_command(\"nvcc --list-gpu-code\").replace('\\n', '\\n  - ')))\n",
    "else:\n",
    "    print(\"System NVCC:     Not found in PATH (This is common if you only use Conda/Pip CUDA)\")\n",
    "\n",
    "driver_ver = run_command(\"nvidia-smi --query-gpu=driver_version --format=csv,noheader\")\n",
    "print(f\"GPU Driver:      {driver_ver}\")\n",
    "\n",
    "visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {visible_devices if visible_devices else 'Not Set (All GPUs visible)'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### PyTorch Bundled with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Debug Build:     {torch.version.debug}\")\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available:   {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # CUDA Version bundled with PyTorch\n",
    "    print(f\"PyTorch CUDA Version:      {torch.version.cuda}\")\n",
    "    print(f\"Architecture List: {torch.cuda.get_arch_list()}\")\n",
    "else:\n",
    "    print(\"\\n[!] CUDA is not available. Please check your NVIDIA drivers and PyTorch installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks available backends for scaled_dot_product_attention (SDPA)\n",
    "sdpa_available = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "if sdpa_available:\n",
    "    print(f\"SDPA Available:     {sdpa_available}\")\n",
    "    # Check which backends are preferred/supported\n",
    "    # (Note: exact API availability depends on specific PyTorch nightly/version)\n",
    "    try:\n",
    "        from torch.backends import cuda as cuda_backend\n",
    "        print(f\"Flash Attention:    {cuda_backend.flash_sdp_enabled()}\")\n",
    "        print(f\"Mem-Efficient Attn: {cuda_backend.mem_efficient_sdp_enabled()}\")\n",
    "        print(f\"Math (Fallback):    {cuda_backend.math_sdp_enabled()}\")\n",
    "    except ImportError:\n",
    "        print(\"Could not query specific SDPA backends.\")\n",
    "else:\n",
    "    print(\"SDPA Available:     No (Upgrade PyTorch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_available = torch.distributed.is_available()\n",
    "if distributed_available:\n",
    "    print(f\"Distributed Available: {distributed_available}\")\n",
    "    \n",
    "    # 1. NCCL Check (NVIDIA Collective Communications Library)\n",
    "    # Note: NCCL is generally NOT available on Windows (it's Linux-only)\n",
    "    nccl_available = hasattr(torch.backends, \"nccl\") and torch.backends.nccl.is_built()\n",
    "    print(f\"NCCL Available:      {nccl_available}\")\n",
    "    if nccl_available:\n",
    "        try:\n",
    "            print(f\"NCCL Version:        {torch.cuda.nccl.version()}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # 2. Gloo Check (Facebook's collective library - standard for Windows)\n",
    "    gloo_available = hasattr(torch.distributed, \"is_gloo_available\")\n",
    "    print(f\"Gloo Available:      {gloo_available}\")\n",
    "    \n",
    "    # 3. MPI Check (Message Passing Interface)\n",
    "    mpi_available = torch.distributed.is_mpi_available()\n",
    "    print(f\"MPI Available:       {mpi_available}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Distributed Available: {distributed_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## DETECTED DEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Detected by System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_command(\"nvidia-smi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Detected by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda_available:\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Detected GPUs:   {device_count}\")\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\n[GPU {i}: {props.name}]\")\n",
    "        print(f\"  Compute Capability:  {props.major}.{props.minor}\")\n",
    "        print(f\"  Total Memory:        {props.total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"  Multiprocessors:     {props.multi_processor_count}\")\n",
    "        \n",
    "        # Precision Support Checks\n",
    "        try:\n",
    "            # TF32 (TensorFloat-32) - Ampere+ only\n",
    "            print(f\"  TF32 Allowed (Matmul): {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "            print(f\"  TF32 Allowed (CuDNN):  {torch.backends.cudnn.allow_tf32}\")\n",
    "            \n",
    "            # BF16 (BFloat16) - Ampere+ only\n",
    "            print(f\"  BF16 Supported:        {torch.cuda.is_bf16_supported()}\")\n",
    "            \n",
    "            # FP16 (Half Precision)\n",
    "            # Generally supported on all modern GPUs, but good to check context\n",
    "            hw_fp16 = \"YES (Native)\" if props.major >= 6 else \"Partial (Storage Only/Slow)\"\n",
    "            if props.major >= 7:\n",
    "                hw_fp16 += \" + Tensor Cores\"\n",
    "            print(f\"  FP16 Support (HW):   {hw_fp16}\")\n",
    "        except AttributeError:\n",
    "            print(\"  (Newer precision checks skipped - PyTorch version too old)\")\n",
    "else:\n",
    "    print(\"No CUDA devices found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## FUNCTIONAL TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda_available:\n",
    "    try:\n",
    "        # Create a tensor on CPU\n",
    "        x = torch.rand(1000, 1000)\n",
    "        print(\"1. Tensor created on CPU.\")\n",
    "        \n",
    "        # Move to GPU\n",
    "        device = torch.device(\"cuda\")\n",
    "        x_gpu = x.to(device)\n",
    "        print(f\"2. Tensor successfully moved to: {x_gpu.device}\")\n",
    "        \n",
    "        # Perform Operation (Matrix Multiplication)\n",
    "        y_gpu = torch.matmul(x_gpu, x_gpu)\n",
    "        print(\"3. Matrix multiplication on GPU successful.\")\n",
    "        \n",
    "        # Move back to CPU\n",
    "        y_cpu = y_gpu.cpu()\n",
    "        print(\"4. Result moved back to CPU.\")\n",
    "        print(\"[OK] PyTorch CUDA functionality is working.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[!] ERROR during functional test:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## MEMORY DIAGNOSTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda_available:\n",
    "    # Current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    print(f\"Current Memory Allocated: {allocated:.4f} GB\")\n",
    "    print(f\"Current Memory Reserved:  {reserved:.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=0, abbreviated=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
