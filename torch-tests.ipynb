{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dd576f",
   "metadata": {},
   "source": [
    "# PYTORCH - CUDA CHECKLIST NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376adefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab2b78",
   "metadata": {},
   "source": [
    "## SYSTEM & VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b64b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:    3.12.10\n",
      "PyTorch Version:   2.8.0+cu129\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python Version:    {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch Version:   {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62074e",
   "metadata": {},
   "source": [
    "### 1. CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:    True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available:    {cuda_available}\")\n",
    "\n",
    "if not cuda_available:\n",
    "    print(\"\\n[!] CUDA is not available. Please check your NVIDIA drivers and PyTorch installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b348e46",
   "metadata": {},
   "source": [
    "### 2. Versions and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e93be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version:      12.9\n",
      "CuDNN Version:     91002\n",
      "CuDNN Enabled:     True\n",
      "Architecture List: ['sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120']\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    print(f\"CUDA Version:      {torch.version.cuda}\")\n",
    "    print(f\"CuDNN Version:     {torch.backends.cudnn.version()}\")   \n",
    "    print(f\"CuDNN Enabled:     {torch.backends.cudnn.enabled}\")\n",
    "    print(f\"Architecture List: {torch.cuda.get_arch_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045d8cc",
   "metadata": {},
   "source": [
    "## DETECTED DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d63a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  - Compute Capability:  8.9\n",
      "  - Total Memory:        8.00 GB\n",
      "  - Multi-Processor Cnt: 24\n",
      "  - BF16 Support:        True\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    device_count = torch.cuda.device_count()\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {props.name}\")\n",
    "        print(f\"  - Compute Capability:  {props.major}.{props.minor}\")\n",
    "        print(f\"  - Total Memory:        {props.total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"  - Multi-Processor Cnt: {props.multi_processor_count}\")\n",
    "        print(f\"  - BF16 Support:        {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8934c5",
   "metadata": {},
   "source": [
    "## FUNCTIONAL TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e89cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tensor created on CPU.\n",
      "2. Tensor successfully moved to: cuda:0\n",
      "3. Matrix multiplication on GPU successful.\n",
      "4. Result moved back to CPU.\n",
      "[OK] PyTorch CUDA functionality is working.\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    try:\n",
    "        # Create a tensor on CPU\n",
    "        x = torch.rand(1000, 1000)\n",
    "        print(\"1. Tensor created on CPU.\")\n",
    "        \n",
    "        # Move to GPU\n",
    "        device = torch.device(\"cuda\")\n",
    "        x_gpu = x.to(device)\n",
    "        print(f\"2. Tensor successfully moved to: {x_gpu.device}\")\n",
    "        \n",
    "        # Perform Operation (Matrix Multiplication)\n",
    "        y_gpu = torch.matmul(x_gpu, x_gpu)\n",
    "        print(\"3. Matrix multiplication on GPU successful.\")\n",
    "        \n",
    "        # Move back to CPU\n",
    "        y_cpu = y_gpu.cpu()\n",
    "        print(\"4. Result moved back to CPU.\")\n",
    "        print(\"[OK] PyTorch CUDA functionality is working.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[!] ERROR during functional test:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413db5d",
   "metadata": {},
   "source": [
    "## MEMORY DIAGNOSTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06e0742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Memory Allocated: 0.0154 GB\n",
      "Current Memory Reserved:  0.0195 GB\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    # Current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    print(f\"Current Memory Allocated: {allocated:.4f} GB\")\n",
    "    print(f\"Current Memory Reserved:  {reserved:.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3690a893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  16133 KiB |  16133 KiB |  16133 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  16133 KiB |  16133 KiB |  16133 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  16132 KiB |  16132 KiB |  16132 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   4347 KiB |  16573 KiB |  16573 KiB |  12226 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=0, abbreviated=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
